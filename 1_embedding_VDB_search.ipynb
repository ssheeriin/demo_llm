{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings, Vector Databases, and Search\n",
    "\n",
    "Converting text into embedding vectors is the first step to any text processing pipeline. As the amount of text gets larger, there is often a need to save these embedding vectors into a dedicated vector index or library, so that developers won't have to recompute the embeddings and the retrieval process is faster. We can then search for documents based on our intended query and pass these relevant documents into a language model (LM) as additional context. We also refer to this context as supplying the LM with \"state\" or \"memory\". The LM then generates a response based on the additional context it receives! \n",
    "\n",
    "In this notebook, we will implement the full workflow of text vectorization, vector search, and question answering workflow. While we use [FAISS](https://faiss.ai/) (vector library) and [ChromaDB](https://docs.trychroma.com/) (vector database), and a Hugging Face model, know that you can easily swap these tools out for your preferred tools or models!\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/updated_vector_search.png\" width=1000 target=\"_blank\" > \n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Implement the workflow of reading text, converting text to embeddings, saving them to FAISS and ChromaDB \n",
    "2. Query for similar documents using FAISS and ChromaDB \n",
    "3. Apply a Hugging Face language model for question answering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/lib/python3.10/site-packages (1.7.4)\n",
      "Requirement already satisfied: chromadb in /opt/homebrew/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.21.tar.gz (22.3 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/22.3 MB 7.0 MB/s eta 0:00:001\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.28 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (1.10.9)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (0.7.2)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (1.24.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (4.6.3)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/homebrew/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.28->chromadb) (2.0.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/homebrew/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.1)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xformers: filename=xformers-0.0.21-cp310-cp310-macosx_13_0_arm64.whl size=468612 sha256=ed6ea3d6310e8783d79f01abbee2fa6ee365708a2496854b6a28cef5832bc788\n",
      "  Stored in directory: /Users/i073367/Library/Caches/pip/wheels/c4/90/13/2d72334071ad319d561fe76bd540945d3075e010fdbcd2fb27\n",
      "Successfully built xformers\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.21\n"
     ]
    }
   ],
   "source": [
    "!pip3 install faiss-cpu chromadb pandas sentence-transformers xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "Read and print the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.eurekalert.org/pub_releases/2020-0...</td>\n",
       "      <td>eurekalert.org</td>\n",
       "      <td>2020-08-06 13:59:45</td>\n",
       "      <td>A closer look at water-splitting's solar fuel ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.pulse.ng/news/world/an-irresistibl...</td>\n",
       "      <td>pulse.ng</td>\n",
       "      <td>2020-08-12 15:14:19</td>\n",
       "      <td>An irresistible scent makes locusts swarm, stu...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.express.co.uk/news/science/1322607...</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>2020-08-13 21:01:00</td>\n",
       "      <td>Artificial intelligence warning: AI will know ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.ndtv.com/world-news/glaciers-could...</td>\n",
       "      <td>ndtv.com</td>\n",
       "      <td>2020-08-03 22:18:26</td>\n",
       "      <td>Glaciers Could Have Sculpted Mars Valleys: Study</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.thesun.ie/tech/5742187/perseid-met...</td>\n",
       "      <td>thesun.ie</td>\n",
       "      <td>2020-08-12 19:54:36</td>\n",
       "      <td>Perseid meteor shower 2020: What time and how ...</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108769</th>\n",
       "      <td>NATION</td>\n",
       "      <td>https://www.vanguardngr.com/2020/08/pdp-govern...</td>\n",
       "      <td>vanguardngr.com</td>\n",
       "      <td>2020-08-08 02:40:00</td>\n",
       "      <td>PDP governors’ forum urges security agencies t...</td>\n",
       "      <td>en</td>\n",
       "      <td>108769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108770</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>https://www.patentlyapple.com/patently-apple/2...</td>\n",
       "      <td>patentlyapple.com</td>\n",
       "      <td>2020-08-08 01:27:12</td>\n",
       "      <td>In Q2-20, Apple Dominated the Premium Smartpho...</td>\n",
       "      <td>en</td>\n",
       "      <td>108770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108771</th>\n",
       "      <td>HEALTH</td>\n",
       "      <td>https://www.belfastlive.co.uk/news/health/coro...</td>\n",
       "      <td>belfastlive.co.uk</td>\n",
       "      <td>2020-08-12 17:01:00</td>\n",
       "      <td>Coronavirus Northern Ireland: Full breakdown s...</td>\n",
       "      <td>en</td>\n",
       "      <td>108771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108772</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>https://www.thenews.com.pk/latest/696364-paul-...</td>\n",
       "      <td>thenews.com.pk</td>\n",
       "      <td>2020-08-05 04:59:00</td>\n",
       "      <td>Paul McCartney details post-Beatles distress a...</td>\n",
       "      <td>en</td>\n",
       "      <td>108772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108773</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>https://www.balls.ie/football/shane-duffy-brig...</td>\n",
       "      <td>balls.ie</td>\n",
       "      <td>2020-08-09 10:25:26</td>\n",
       "      <td>Report: Talks Underway To Keep Shane Duffy In ...</td>\n",
       "      <td>en</td>\n",
       "      <td>108773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108774 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                topic                                               link  \\\n",
       "0             SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...   \n",
       "1             SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...   \n",
       "2             SCIENCE  https://www.express.co.uk/news/science/1322607...   \n",
       "3             SCIENCE  https://www.ndtv.com/world-news/glaciers-could...   \n",
       "4             SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...   \n",
       "...               ...                                                ...   \n",
       "108769         NATION  https://www.vanguardngr.com/2020/08/pdp-govern...   \n",
       "108770       BUSINESS  https://www.patentlyapple.com/patently-apple/2...   \n",
       "108771         HEALTH  https://www.belfastlive.co.uk/news/health/coro...   \n",
       "108772  ENTERTAINMENT  https://www.thenews.com.pk/latest/696364-paul-...   \n",
       "108773         SPORTS  https://www.balls.ie/football/shane-duffy-brig...   \n",
       "\n",
       "                   domain       published_date  \\\n",
       "0          eurekalert.org  2020-08-06 13:59:45   \n",
       "1                pulse.ng  2020-08-12 15:14:19   \n",
       "2           express.co.uk  2020-08-13 21:01:00   \n",
       "3                ndtv.com  2020-08-03 22:18:26   \n",
       "4               thesun.ie  2020-08-12 19:54:36   \n",
       "...                   ...                  ...   \n",
       "108769    vanguardngr.com  2020-08-08 02:40:00   \n",
       "108770  patentlyapple.com  2020-08-08 01:27:12   \n",
       "108771  belfastlive.co.uk  2020-08-12 17:01:00   \n",
       "108772     thenews.com.pk  2020-08-05 04:59:00   \n",
       "108773           balls.ie  2020-08-09 10:25:26   \n",
       "\n",
       "                                                    title lang      id  \n",
       "0       A closer look at water-splitting's solar fuel ...   en       0  \n",
       "1       An irresistible scent makes locusts swarm, stu...   en       1  \n",
       "2       Artificial intelligence warning: AI will know ...   en       2  \n",
       "3        Glaciers Could Have Sculpted Mars Valleys: Study   en       3  \n",
       "4       Perseid meteor shower 2020: What time and how ...   en       4  \n",
       "...                                                   ...  ...     ...  \n",
       "108769  PDP governors’ forum urges security agencies t...   en  108769  \n",
       "108770  In Q2-20, Apple Dominated the Premium Smartpho...   en  108770  \n",
       "108771  Coronavirus Northern Ireland: Full breakdown s...   en  108771  \n",
       "108772  Paul McCartney details post-Beatles distress a...   en  108772  \n",
       "108773  Report: Talks Underway To Keep Shane Duffy In ...   en  108773  \n",
       "\n",
       "[108774 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_csv(f\"./documents/news/labelled_newscatcher_dataset.csv\", sep=\";\")\n",
    "pdf[\"id\"] = pdf.index\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Library: FAISS\n",
    "\n",
    "Vector libraries are often sufficient for small, static data. Since it's not a full-fledged database solution, it doesn't have the CRUD (Create, Read, Update, Delete) support. Once the index has been built, if there are more vectors that need to be added/removed/edited, the index has to be rebuilt from scratch. \n",
    "\n",
    "That said, vector libraries are easy, lightweight, and fast to use. Examples of vector libraries are [FAISS](https://faiss.ai/), [ScaNN](https://github.com/google-research/google-research/tree/master/scann), [ANNOY](https://github.com/spotify/annoy), and [HNSM](https://arxiv.org/abs/1603.09320).\n",
    "\n",
    "FAISS has several ways for similarity search: L2 (Euclidean distance), cosine similarity. You can read more about their implementation on their [GitHub](https://github.com/facebookresearch/faiss/wiki/Getting-started#searching) page or [blog post](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/). They also published their own [best practice guide here](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index).\n",
    "\n",
    "If you'd like to read up more on the comparisons between vector libraries and databases, [here is a good blog post](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall workflow of FAISS is captured in the diagram below. \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*ouf0eyQskPeGWIGm\" width=700>\n",
    "\n",
    "Source: [How to use FAISS to build your first similarity search by Asna Shafiq](https://medium.com/loopio-tech/how-to-use-faiss-to-build-your-first-similarity-search-bf0f708aa772)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "pdf_subset = pdf.head(1000 ) # use only the first 1000 rows for performance \n",
    "\n",
    "def example_create_fn(doc1: pd.Series) -> InputExample:\n",
    "    \"\"\"\n",
    "    Helper function that outputs a sentence_transformer guid, label, and text\n",
    "    \"\"\"\n",
    "    return InputExample(texts=[doc1])\n",
    "\n",
    "faiss_train_examples = pdf_subset.apply(               \n",
    "    lambda x: example_create_fn(x[\"title\"]), axis=1\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Vectorize text into embedding vectors\n",
    "We will be using `Sentence-Transformers` [library](https://www.sbert.net/) to load a language model to vectorize our text into embeddings. The library hosts some of the most popular transformers on [Hugging Face Model Hub](https://huggingface.co/sentence-transformers).\n",
    "Here, we are using the `model = SentenceTransformer(\"all-MiniLM-L6-v2\")` to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 384)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", \n",
    "    cache_folder=\"cache\"\n",
    ")  # Use a pre-cached model\n",
    "faiss_title_embedding = model.encode(pdf_subset.title.values.tolist())   # do embedding\n",
    "len(faiss_title_embedding), len(faiss_title_embedding[0])  # print no of records, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11270543  0.04076544  0.0218142  ... -0.01874593 -0.03136874\n",
      "   0.06824827]\n",
      " [-0.02187166 -0.03349999  0.07321803 ...  0.03362316 -0.00563881\n",
      "  -0.00630975]\n",
      " [ 0.01608377  0.00279449 -0.01504422 ... -0.00706243  0.00905903\n",
      "  -0.02835057]\n",
      " ...\n",
      " [ 0.01506927  0.04583013 -0.06114494 ... -0.07814182 -0.08025029\n",
      "   0.01337817]\n",
      " [-0.07082239  0.00643819  0.00809315 ... -0.05520815 -0.03652048\n",
      "   0.07594126]\n",
      " [-0.06321973  0.0446152  -0.07385815 ...  0.06559423  0.03276755\n",
      "   0.09071001]]\n"
     ]
    }
   ],
   "source": [
    "print(faiss_title_embedding )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Saving embedding vectors to FAISS index\n",
    "Below, we create the FAISS index object based on our embedding vectors, normalize vectors, and add these vectors to the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "pdf_to_index = pdf_subset.set_index([\"id\"], drop=False)\n",
    "id_index = np.array(pdf_to_index.id.values).flatten().astype(\"int\")\n",
    "\n",
    "content_encoded_normalized = faiss_title_embedding.copy()\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "# Index1DMap translates search results to IDs: https://faiss.ai/cpp_api/file/IndexIDMap_8h.html#_CPPv4I0EN5faiss18IndexIDMapTemplateE\n",
    "# The IndexFlatIP below builds index\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(len(faiss_title_embedding[0])))\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Search for relevant documents\n",
    "We define a search function below to first vectorize our query text, and then search for the vectors with the closest distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada! Now you can query for similar content! Notice that you did not have to configure any database networks beforehand nor pass in any credentials. FAISS works locally with your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_content(query, pdf_to_index, k=3):\n",
    "    query_vector = model.encode([query])\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # We set k to limit the number of vectors we want to return\n",
    "    top_k = index_content.search(query_vector, k)\n",
    "    ids = top_k[1][0].tolist()\n",
    "    similarities = top_k[0][0].tolist()\n",
    "    results = pdf_to_index.loc[ids]\n",
    "    results[\"similarities\"] = similarities\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(search_content(\"animal\", pdf_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we haven't done the last step of conducting Q/A with a language model yet. We are going to demonstrate this with Chroma, a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database: Chroma\n",
    "Chroma is an open-source embedding database. The company just raised its seed funding in April 2023 and is quickly becoming popular to support LLM-based applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"vdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma Concept: Collection\n",
    "Chroma collection is akin to an index that stores one set of your documents.\n",
    "\n",
    "According to the docs:\n",
    "\n",
    "Collections are where you will store your embeddings, documents, and additional metadata\n",
    "\n",
    "The nice thing about ChromaDB is that if you don't supply a model to vectorize text into embeddings, it will automatically load a default embedding function, i.e. SentenceTransformerEmbeddingFunction. It can handle tokenization, embedding, and indexing automatically for you. If you would like to change the embedding model, read here on how to do that. TLDR: you can add an optional model_name argument.\n",
    "\n",
    "You can read the documentation here on rules for collection names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: 'my_news'\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"my_news\"\n",
    "\n",
    "# If you have created the collection before, you need to delete the collection first\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"Creating collection: '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Add data to collection\n",
    "Since we are re-using the same data, we can skip the step of reading data. As mentioned in the text above, Chroma can take care of text vectorization for us, so we can directly add text to the collection and Chroma will convert the text into embeddings behind the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.eurekalert.org/pub_releases/2020-0...</td>\n",
       "      <td>eurekalert.org</td>\n",
       "      <td>2020-08-06 13:59:45</td>\n",
       "      <td>A closer look at water-splitting's solar fuel ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.pulse.ng/news/world/an-irresistibl...</td>\n",
       "      <td>pulse.ng</td>\n",
       "      <td>2020-08-12 15:14:19</td>\n",
       "      <td>An irresistible scent makes locusts swarm, stu...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.express.co.uk/news/science/1322607...</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>2020-08-13 21:01:00</td>\n",
       "      <td>Artificial intelligence warning: AI will know ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.ndtv.com/world-news/glaciers-could...</td>\n",
       "      <td>ndtv.com</td>\n",
       "      <td>2020-08-03 22:18:26</td>\n",
       "      <td>Glaciers Could Have Sculpted Mars Valleys: Study</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.thesun.ie/tech/5742187/perseid-met...</td>\n",
       "      <td>thesun.ie</td>\n",
       "      <td>2020-08-12 19:54:36</td>\n",
       "      <td>Perseid meteor shower 2020: What time and how ...</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>https://www.androidcentral.com/mate-40-will-be...</td>\n",
       "      <td>androidcentral.com</td>\n",
       "      <td>2020-08-07 17:12:33</td>\n",
       "      <td>The Mate 40 will be the last Huawei phone with...</td>\n",
       "      <td>en</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.cnn.com/2020/08/17/africa/stone-ag...</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>2020-08-17 17:10:00</td>\n",
       "      <td>Early humans knew how to make comfy, pest-free...</td>\n",
       "      <td>en</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>HEALTH</td>\n",
       "      <td>https://www.tenterfieldstar.com.au/story/68776...</td>\n",
       "      <td>tenterfieldstar.com.au</td>\n",
       "      <td>2020-08-13 03:26:06</td>\n",
       "      <td>Regional Vic set for virus testing blitz</td>\n",
       "      <td>en</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>HEALTH</td>\n",
       "      <td>https://news.sky.com/story/coronavirus-trials-...</td>\n",
       "      <td>news.sky.com</td>\n",
       "      <td>2020-08-13 13:22:58</td>\n",
       "      <td>Coronavirus: Trials of second contact-tracing ...</td>\n",
       "      <td>en</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>HEALTH</td>\n",
       "      <td>https://www.techexplorist.com/study-demonstrat...</td>\n",
       "      <td>techexplorist.com</td>\n",
       "      <td>2020-08-10 07:47:00</td>\n",
       "      <td>The study demonstrates new treatment for prion...</td>\n",
       "      <td>en</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic                                               link  \\\n",
       "0       SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...   \n",
       "1       SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...   \n",
       "2       SCIENCE  https://www.express.co.uk/news/science/1322607...   \n",
       "3       SCIENCE  https://www.ndtv.com/world-news/glaciers-could...   \n",
       "4       SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...   \n",
       "..          ...                                                ...   \n",
       "995  TECHNOLOGY  https://www.androidcentral.com/mate-40-will-be...   \n",
       "996     SCIENCE  https://www.cnn.com/2020/08/17/africa/stone-ag...   \n",
       "997      HEALTH  https://www.tenterfieldstar.com.au/story/68776...   \n",
       "998      HEALTH  https://news.sky.com/story/coronavirus-trials-...   \n",
       "999      HEALTH  https://www.techexplorist.com/study-demonstrat...   \n",
       "\n",
       "                     domain       published_date  \\\n",
       "0            eurekalert.org  2020-08-06 13:59:45   \n",
       "1                  pulse.ng  2020-08-12 15:14:19   \n",
       "2             express.co.uk  2020-08-13 21:01:00   \n",
       "3                  ndtv.com  2020-08-03 22:18:26   \n",
       "4                 thesun.ie  2020-08-12 19:54:36   \n",
       "..                      ...                  ...   \n",
       "995      androidcentral.com  2020-08-07 17:12:33   \n",
       "996                 cnn.com  2020-08-17 17:10:00   \n",
       "997  tenterfieldstar.com.au  2020-08-13 03:26:06   \n",
       "998            news.sky.com  2020-08-13 13:22:58   \n",
       "999       techexplorist.com  2020-08-10 07:47:00   \n",
       "\n",
       "                                                 title lang   id  \n",
       "0    A closer look at water-splitting's solar fuel ...   en    0  \n",
       "1    An irresistible scent makes locusts swarm, stu...   en    1  \n",
       "2    Artificial intelligence warning: AI will know ...   en    2  \n",
       "3     Glaciers Could Have Sculpted Mars Valleys: Study   en    3  \n",
       "4    Perseid meteor shower 2020: What time and how ...   en    4  \n",
       "..                                                 ...  ...  ...  \n",
       "995  The Mate 40 will be the last Huawei phone with...   en  995  \n",
       "996  Early humans knew how to make comfy, pest-free...   en  996  \n",
       "997           Regional Vic set for virus testing blitz   en  997  \n",
       "998  Coronavirus: Trials of second contact-tracing ...   en  998  \n",
       "999  The study demonstrates new treatment for prion...   en  999  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pdf_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document must have a unique id associated with it and it is up to you to check that there are no duplicate ids.\n",
    "\n",
    "Adding data to collection will take some time to run, especially when there is a lot of data. In the cell below, we intentionally write only a subset of data to the collection to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=pdf_subset[\"title\"][:100].tolist(),\n",
    "    metadatas=[{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(100)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Query for 10 relevant documents on \"space\"\n",
    "We will return 10 most relevant documents. You can think of 10 as 10 nearest neighbors. You can also change the number of results returned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ids\": [\n",
      "        [\n",
      "            \"id72\",\n",
      "            \"id7\",\n",
      "            \"id30\",\n",
      "            \"id26\",\n",
      "            \"id23\",\n",
      "            \"id76\",\n",
      "            \"id69\",\n",
      "            \"id40\",\n",
      "            \"id47\",\n",
      "            \"id75\"\n",
      "        ]\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        [\n",
      "            1.2250351905822754,\n",
      "            1.3089768886566162,\n",
      "            1.3910387754440308,\n",
      "            1.4064621925354004,\n",
      "            1.4391299486160278,\n",
      "            1.4898797273635864,\n",
      "            1.5728241205215454,\n",
      "            1.5738130807876587,\n",
      "            1.583530068397522,\n",
      "            1.5864627361297607\n",
      "        ]\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        [\n",
      "            {\n",
      "                \"topic\": \"TECHNOLOGY\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            },\n",
      "            {\n",
      "                \"topic\": \"SCIENCE\"\n",
      "            }\n",
      "        ]\n",
      "    ],\n",
      "    \"embeddings\": null,\n",
      "    \"documents\": [\n",
      "        [\n",
      "            \"Beck teams up with NASA and AI for 'Hyperspace' visual album experience\",\n",
      "            \"Orbital space tourism set for rebirth in 2021\",\n",
      "            \"NASA drops \\\"insensitive\\\" nicknames for cosmic objects\",\n",
      "            \"\\u2018It came alive:\\u2019 NASA astronauts describe experiencing splashdown in SpaceX Dragon\",\n",
      "            \"Hubble Uses Moon As \\u201cMirror\\u201d to Study Earth\\u2019s Atmosphere \\u2013 Proxy in Search of Potentially Habitable Planets Around Other Stars\",\n",
      "            \"Australia's small yet crucial part in the mission to find life on Mars\",\n",
      "            \"NASA Astronauts in SpaceX Capsule Splashdown in Gulf Of Mexico\",\n",
      "            \"SpaceX's Starship spacecraft saw 150 meters high\",\n",
      "            \"NASA\\u2019s InSight lander shows what\\u2019s beneath Mars\\u2019 surface\",\n",
      "            \"Alien base on Mercury: ET hunters claim to find huge UFO\"\n",
      "        ]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = collection.query(query_texts=[\"space\"], n_results=10)\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Add filter statement\n",
    "In addition to conducting relevancy search, we can also add filter statements. Refer to the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_texts=[\"space\"], where={\"topic\": \"SCIENCE\"}, n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Update data in a collection\n",
    "Unlike a vector library, vector databases support changes to the data so we can update or delete data.\n",
    "\n",
    "Indeed, we can update or delete data in a Chroma collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete(ids=[\"id0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The record with ids=0 is no longer present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    ids=[\"id0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also update a specific data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    ids=[\"id2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.update(\n",
    "    ids=[\"id2\"],\n",
    "    metadatas=[{\"topic\": \"TECHNOLOGY\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering for question answering\n",
    "Now that we have identified documents about space from the news dataset, we can pass these documents as additional context for a language model to generate a response based on them!\n",
    "\n",
    "We first need to pick a text-generation model. Below, we use a Hugging Face model. You can also use OpenAI as well, but you will need to get an Open AI token and pay based on the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='cache')\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir='cache')\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where prompt engineering, which is developing prompts, comes in. We pass in the context in our prompt_template but there are numerous ways to write a prompt. Some prompts may generate better results than the others and it requires some experimentation to figure out how best to talk to the model. Each language model behaves differently to prompts.\n",
    "\n",
    "Our prompt template below is inspired from a [2023 paper on program-aided language model](https://arxiv.org/pdf/2211.10435.pdf). The authors have provided their sample prompt template [here](https://github.com/reasoning-machines/pal/blob/main/pal/prompt/date_understanding_prompt.py).\n",
    "\n",
    "The following links also provide some helpful guidance on prompt engineering:\n",
    "\n",
    "[Prompt engineering with OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
    "\n",
    "[GitHub repo that compiles best practices to interact with ChatGPT](https://github.com/f/awesome-chatgpt-prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's the latest news on space development?\"\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant context: #Beck teams up with NASA and AI for 'Hyperspace' visual album experience #Orbital space tourism set for rebirth in 2021 #NASA drops \"insensitive\" nicknames for cosmic objects #‘It came alive:’ NASA astronauts describe experiencing splashdown in SpaceX Dragon #Hubble Uses Moon As “Mirror” to Study Earth’s Atmosphere – Proxy in Search of Potentially Habitable Planets Around Other Stars #Australia's small yet crucial part in the mission to find life on Mars #NASA Astronauts in SpaceX Capsule Splashdown in Gulf Of Mexico #SpaceX's Starship spacecraft saw 150 meters high #NASA’s InSight lander shows what’s beneath Mars’ surface #Alien base on Mercury: ET hunters claim to find huge UFO\n",
      "\n",
      " The user's question: What's the latest news on space development? The visitor's question: If the space world doesn't look this good next to us, is it time for an astronaut to join your spaceship?\n",
      "\n",
      "The answer, according to the recent survey, is \"yes\".\n",
      "\n",
      "It turns out that a major chunk of NASA's budget comes from the U.S. taxpayer.\n",
      "\n",
      "\n",
      "Last week, NASA announced plans to spend $5.6 billion on the commercial space industry for FY2018 and 2020.\n",
      "\n",
      "The average cost per manned mission for FY2018 is $7,280 per mission (not including commercial operators), which will increase from $7,150 for FY2017 and $8,500 for FY2020.\n",
      "\n",
      "Here again, the answer is \"yes\".\n",
      "\n",
      "In fact, there was a big increase to the current budget on the first five years of the program.\n",
      "\n",
      "The current budget covers the first 12 years of programs, which lasts through the next decade.\n",
      "\n",
      "NASA received $8,051,895 in FY2019 in FY2023, which will be the second time in 10 years that such an increase has happened.\n",
      "\n",
      "And as of FY2023, NASA received the highest federal-university funding (of $4.6 million) in space since NASA's inception in the late 60s.\n",
      "\n",
      "This was one of the more dramatic changes that occurred during the 20th century.\n",
      "\n",
      "Last year, NASA spent $16.2 billion on a $1.1 trillion program to develop, develop and test low-cost space programs, according to the U.S. Government Accountability Office.\n",
      "\n",
      "A $50 billion mission to develop and launch a low-cost space launch vehicle was also a new initiative of the agency.\n",
      "\n",
      "NASA has the highest rate of utilization of space programs since the Reagan era, according to the GAG-SEO data.\n",
      "\n",
      "Even though the space industry was at a dead end, the Space Technology Corporation, which is now a $1.5 billion company owned by the State Department, is now a $5.2 billion company.\n",
      "\n",
      "And the most important change from the 1960s was the change in the way NASA's budget was spent.\n",
      "\n",
      "While the overall budget for FY2020 (which will continue to grow) was $19 billion dollars (6,500,000 in FY2019) and $20 years earlier (since the 1970s), the \"space\" spending rate of 41% in FY2020 was so high NASA only released the FY2020 estimates on the\n"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, you have just completed the implementation of your first text vectorization, search, and question answering workflow (that requires prompt engineering)!\n",
    "\n",
    "In the lab, you will apply your newly gained knowledge to a different dataset. You can also check out the optional modules on Pinecone and Weaviate to learn how to set up vector databases that offer enterprise offerings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
